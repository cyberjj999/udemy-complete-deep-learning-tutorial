{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lP6JLo1tGNBg"
      },
      "source": [
        "# Artificial Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gWZyYmS_UE_L"
      },
      "source": [
        "### Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.13.0'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1E0Q3aoKUCRX"
      },
      "source": [
        "## Part 1 - Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cKWAkFVGUU0Z"
      },
      "source": [
        "### Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = pd.read_csv('Churn_Modelling.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RowNumber</th>\n",
              "      <th>CustomerId</th>\n",
              "      <th>Surname</th>\n",
              "      <th>CreditScore</th>\n",
              "      <th>Geography</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>Tenure</th>\n",
              "      <th>Balance</th>\n",
              "      <th>NumOfProducts</th>\n",
              "      <th>HasCrCard</th>\n",
              "      <th>IsActiveMember</th>\n",
              "      <th>EstimatedSalary</th>\n",
              "      <th>Exited</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>15634602</td>\n",
              "      <td>Hargrave</td>\n",
              "      <td>619</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>42</td>\n",
              "      <td>2</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>101348.88</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>15647311</td>\n",
              "      <td>Hill</td>\n",
              "      <td>608</td>\n",
              "      <td>Spain</td>\n",
              "      <td>Female</td>\n",
              "      <td>41</td>\n",
              "      <td>1</td>\n",
              "      <td>83807.86</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>112542.58</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>15619304</td>\n",
              "      <td>Onio</td>\n",
              "      <td>502</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>42</td>\n",
              "      <td>8</td>\n",
              "      <td>159660.80</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113931.57</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>15701354</td>\n",
              "      <td>Boni</td>\n",
              "      <td>699</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>39</td>\n",
              "      <td>1</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>93826.63</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>15737888</td>\n",
              "      <td>Mitchell</td>\n",
              "      <td>850</td>\n",
              "      <td>Spain</td>\n",
              "      <td>Female</td>\n",
              "      <td>43</td>\n",
              "      <td>2</td>\n",
              "      <td>125510.82</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>79084.10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>9996</td>\n",
              "      <td>15606229</td>\n",
              "      <td>Obijiaku</td>\n",
              "      <td>771</td>\n",
              "      <td>France</td>\n",
              "      <td>Male</td>\n",
              "      <td>39</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>96270.64</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>9997</td>\n",
              "      <td>15569892</td>\n",
              "      <td>Johnstone</td>\n",
              "      <td>516</td>\n",
              "      <td>France</td>\n",
              "      <td>Male</td>\n",
              "      <td>35</td>\n",
              "      <td>10</td>\n",
              "      <td>57369.61</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>101699.77</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>9998</td>\n",
              "      <td>15584532</td>\n",
              "      <td>Liu</td>\n",
              "      <td>709</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>36</td>\n",
              "      <td>7</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>42085.58</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>9999</td>\n",
              "      <td>15682355</td>\n",
              "      <td>Sabbatini</td>\n",
              "      <td>772</td>\n",
              "      <td>Germany</td>\n",
              "      <td>Male</td>\n",
              "      <td>42</td>\n",
              "      <td>3</td>\n",
              "      <td>75075.31</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>92888.52</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>10000</td>\n",
              "      <td>15628319</td>\n",
              "      <td>Walker</td>\n",
              "      <td>792</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>28</td>\n",
              "      <td>4</td>\n",
              "      <td>130142.79</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>38190.78</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      RowNumber  CustomerId    Surname  CreditScore Geography  Gender  Age  \\\n",
              "0             1    15634602   Hargrave          619    France  Female   42   \n",
              "1             2    15647311       Hill          608     Spain  Female   41   \n",
              "2             3    15619304       Onio          502    France  Female   42   \n",
              "3             4    15701354       Boni          699    France  Female   39   \n",
              "4             5    15737888   Mitchell          850     Spain  Female   43   \n",
              "...         ...         ...        ...          ...       ...     ...  ...   \n",
              "9995       9996    15606229   Obijiaku          771    France    Male   39   \n",
              "9996       9997    15569892  Johnstone          516    France    Male   35   \n",
              "9997       9998    15584532        Liu          709    France  Female   36   \n",
              "9998       9999    15682355  Sabbatini          772   Germany    Male   42   \n",
              "9999      10000    15628319     Walker          792    France  Female   28   \n",
              "\n",
              "      Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
              "0          2       0.00              1          1               1   \n",
              "1          1   83807.86              1          0               1   \n",
              "2          8  159660.80              3          1               0   \n",
              "3          1       0.00              2          0               0   \n",
              "4          2  125510.82              1          1               1   \n",
              "...      ...        ...            ...        ...             ...   \n",
              "9995       5       0.00              2          1               0   \n",
              "9996      10   57369.61              1          1               1   \n",
              "9997       7       0.00              1          0               1   \n",
              "9998       3   75075.31              2          1               0   \n",
              "9999       4  130142.79              1          1               0   \n",
              "\n",
              "      EstimatedSalary  Exited  \n",
              "0           101348.88       1  \n",
              "1           112542.58       0  \n",
              "2           113931.57       1  \n",
              "3            93826.63       0  \n",
              "4            79084.10       0  \n",
              "...               ...     ...  \n",
              "9995         96270.64       0  \n",
              "9996        101699.77       0  \n",
              "9997         42085.58       1  \n",
              "9998         92888.52       1  \n",
              "9999         38190.78       0  \n",
              "\n",
              "[10000 rows x 14 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# we see that the dataset has some useless features that won't help our prediction\n",
        "# i.e. row number, customerID, surname, etc.\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check for Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      RowNumber  CustomerId  Surname  CreditScore  Geography  Gender    Age  \\\n",
            "0         False       False    False        False      False   False  False   \n",
            "1         False       False    False        False      False   False  False   \n",
            "2         False       False    False        False      False   False  False   \n",
            "3         False       False    False        False      False   False  False   \n",
            "4         False       False    False        False      False   False  False   \n",
            "...         ...         ...      ...          ...        ...     ...    ...   \n",
            "9995      False       False    False        False      False   False  False   \n",
            "9996      False       False    False        False      False   False  False   \n",
            "9997      False       False    False        False      False   False  False   \n",
            "9998      False       False    False        False      False   False  False   \n",
            "9999      False       False    False        False      False   False  False   \n",
            "\n",
            "      Tenure  Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
            "0      False    False          False      False           False   \n",
            "1      False    False          False      False           False   \n",
            "2      False    False          False      False           False   \n",
            "3      False    False          False      False           False   \n",
            "4      False    False          False      False           False   \n",
            "...      ...      ...            ...        ...             ...   \n",
            "9995   False    False          False      False           False   \n",
            "9996   False    False          False      False           False   \n",
            "9997   False    False          False      False           False   \n",
            "9998   False    False          False      False           False   \n",
            "9999   False    False          False      False           False   \n",
            "\n",
            "      EstimatedSalary  Exited  \n",
            "0               False   False  \n",
            "1               False   False  \n",
            "2               False   False  \n",
            "3               False   False  \n",
            "4               False   False  \n",
            "...               ...     ...  \n",
            "9995            False   False  \n",
            "9996            False   False  \n",
            "9997            False   False  \n",
            "9998            False   False  \n",
            "9999            False   False  \n",
            "\n",
            "[10000 rows x 14 columns]\n"
          ]
        }
      ],
      "source": [
        "print(dataset.isnull())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RowNumber          0\n",
            "CustomerId         0\n",
            "Surname            0\n",
            "CreditScore        0\n",
            "Geography          0\n",
            "Gender             0\n",
            "Age                0\n",
            "Tenure             0\n",
            "Balance            0\n",
            "NumOfProducts      0\n",
            "HasCrCard          0\n",
            "IsActiveMember     0\n",
            "EstimatedSalary    0\n",
            "Exited             0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# there are no missing data\n",
        "print(dataset.isnull().sum())  # This will print the total number of missing values per column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# iloc: This is an indexer in Pandas used for integer-location based indexing/selection by position.\n",
        "# [:, :-1]: This is the indexing operation being performed by iloc.\n",
        "# The first parameter with a single colon \":\" specifies that all rows should be selected.\n",
        "# The second parameter with \"3:-1\" specifies that all columns from the 4th until except the last one should be selected.\n",
        "# we skip the first 3 columns because they are useless for our prediction - i.e. customerId, rowNumber, surname, etc.\n",
        "# .values: This converts the selected data into a NumPy array.\n",
        "X = dataset.iloc[:, 3:-1].values\n",
        "y = dataset.iloc[:, -1].values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RowNumber</th>\n",
              "      <th>CustomerId</th>\n",
              "      <th>Surname</th>\n",
              "      <th>CreditScore</th>\n",
              "      <th>Geography</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>Tenure</th>\n",
              "      <th>Balance</th>\n",
              "      <th>NumOfProducts</th>\n",
              "      <th>HasCrCard</th>\n",
              "      <th>IsActiveMember</th>\n",
              "      <th>EstimatedSalary</th>\n",
              "      <th>Exited</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>15634602</td>\n",
              "      <td>Hargrave</td>\n",
              "      <td>619</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>42</td>\n",
              "      <td>2</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>101348.88</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>15647311</td>\n",
              "      <td>Hill</td>\n",
              "      <td>608</td>\n",
              "      <td>Spain</td>\n",
              "      <td>Female</td>\n",
              "      <td>41</td>\n",
              "      <td>1</td>\n",
              "      <td>83807.86</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>112542.58</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>15619304</td>\n",
              "      <td>Onio</td>\n",
              "      <td>502</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>42</td>\n",
              "      <td>8</td>\n",
              "      <td>159660.80</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113931.57</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>15701354</td>\n",
              "      <td>Boni</td>\n",
              "      <td>699</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>39</td>\n",
              "      <td>1</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>93826.63</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>15737888</td>\n",
              "      <td>Mitchell</td>\n",
              "      <td>850</td>\n",
              "      <td>Spain</td>\n",
              "      <td>Female</td>\n",
              "      <td>43</td>\n",
              "      <td>2</td>\n",
              "      <td>125510.82</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>79084.10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>9996</td>\n",
              "      <td>15606229</td>\n",
              "      <td>Obijiaku</td>\n",
              "      <td>771</td>\n",
              "      <td>France</td>\n",
              "      <td>Male</td>\n",
              "      <td>39</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>96270.64</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>9997</td>\n",
              "      <td>15569892</td>\n",
              "      <td>Johnstone</td>\n",
              "      <td>516</td>\n",
              "      <td>France</td>\n",
              "      <td>Male</td>\n",
              "      <td>35</td>\n",
              "      <td>10</td>\n",
              "      <td>57369.61</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>101699.77</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>9998</td>\n",
              "      <td>15584532</td>\n",
              "      <td>Liu</td>\n",
              "      <td>709</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>36</td>\n",
              "      <td>7</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>42085.58</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>9999</td>\n",
              "      <td>15682355</td>\n",
              "      <td>Sabbatini</td>\n",
              "      <td>772</td>\n",
              "      <td>Germany</td>\n",
              "      <td>Male</td>\n",
              "      <td>42</td>\n",
              "      <td>3</td>\n",
              "      <td>75075.31</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>92888.52</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>10000</td>\n",
              "      <td>15628319</td>\n",
              "      <td>Walker</td>\n",
              "      <td>792</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>28</td>\n",
              "      <td>4</td>\n",
              "      <td>130142.79</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>38190.78</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      RowNumber  CustomerId    Surname  CreditScore Geography  Gender  Age  \\\n",
              "0             1    15634602   Hargrave          619    France  Female   42   \n",
              "1             2    15647311       Hill          608     Spain  Female   41   \n",
              "2             3    15619304       Onio          502    France  Female   42   \n",
              "3             4    15701354       Boni          699    France  Female   39   \n",
              "4             5    15737888   Mitchell          850     Spain  Female   43   \n",
              "...         ...         ...        ...          ...       ...     ...  ...   \n",
              "9995       9996    15606229   Obijiaku          771    France    Male   39   \n",
              "9996       9997    15569892  Johnstone          516    France    Male   35   \n",
              "9997       9998    15584532        Liu          709    France  Female   36   \n",
              "9998       9999    15682355  Sabbatini          772   Germany    Male   42   \n",
              "9999      10000    15628319     Walker          792    France  Female   28   \n",
              "\n",
              "      Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
              "0          2       0.00              1          1               1   \n",
              "1          1   83807.86              1          0               1   \n",
              "2          8  159660.80              3          1               0   \n",
              "3          1       0.00              2          0               0   \n",
              "4          2  125510.82              1          1               1   \n",
              "...      ...        ...            ...        ...             ...   \n",
              "9995       5       0.00              2          1               0   \n",
              "9996      10   57369.61              1          1               1   \n",
              "9997       7       0.00              1          0               1   \n",
              "9998       3   75075.31              2          1               0   \n",
              "9999       4  130142.79              1          1               0   \n",
              "\n",
              "      EstimatedSalary  Exited  \n",
              "0           101348.88       1  \n",
              "1           112542.58       0  \n",
              "2           113931.57       1  \n",
              "3            93826.63       0  \n",
              "4            79084.10       0  \n",
              "...               ...     ...  \n",
              "9995         96270.64       0  \n",
              "9996        101699.77       0  \n",
              "9997         42085.58       1  \n",
              "9998         92888.52       1  \n",
              "9999         38190.78       0  \n",
              "\n",
              "[10000 rows x 14 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[619, 'France', 'Female', ..., 1, 1, 101348.88],\n",
              "       [608, 'Spain', 'Female', ..., 0, 1, 112542.58],\n",
              "       [502, 'France', 'Female', ..., 1, 0, 113931.57],\n",
              "       ...,\n",
              "       [709, 'France', 'Female', ..., 0, 1, 42085.58],\n",
              "       [772, 'Germany', 'Male', ..., 1, 0, 92888.52],\n",
              "       [792, 'France', 'Female', ..., 1, 0, 38190.78]], dtype=object)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# see how this data ignores the first 3 column which have useless data like row number, customerID, surname, etc.\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 0, 1, ..., 1, 1, 0], dtype=int64)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N6bQ0UgSU-NJ"
      },
      "source": [
        "### Encoding categorical data\n",
        "- we realize our dataset has two categorical data, namely country and gender\n",
        "- so we need to label them\n",
        "  (transform a list of labels into encoded numeric values. This is often necessary because machine learning algorithms generally work with numeric data, so labels like strings or other types need to be converted into numbers.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Label Encoding Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original labels:\n",
            "['cat', 'dog', 'bird', 'cat', 'bird', 'dog']\n",
            "\n",
            "Encoded labels:\n",
            "[1 2 0 1 0 2]\n",
            "\n",
            "Transformed back labels:\n",
            "['cat' 'dog' 'bird' 'cat' 'bird' 'dog']\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Sample data: a list of labels (could be strings or integers)\n",
        "# For example, these could be categories like 'cat', 'dog', and 'bird'\n",
        "example_y = ['cat', 'dog', 'bird', 'cat', 'bird', 'dog']\n",
        "\n",
        "# Print original labels\n",
        "print(\"Original labels:\")\n",
        "print(example_y)\n",
        "\n",
        "# Create a label encoder object\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Fit the encoder to the labels and transform the labels into encoded integers\n",
        "# The transformation is based on alphabetical order, so 'bird' becomes 0, 'cat' becomes 1, and 'dog' becomes 2\n",
        "example_y_encoded = le.fit_transform(example_y)\n",
        "\n",
        "# Print encoded labels\n",
        "print(\"\\nEncoded labels:\")\n",
        "print(example_y_encoded)\n",
        "\n",
        "# If you want to transform the encoded labels back to the original labels\n",
        "y_original = le.inverse_transform(example_y_encoded)\n",
        "\n",
        "# Print the transformed back labels\n",
        "print(\"\\nTransformed back labels:\")\n",
        "print(y_original)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(6,)\n"
          ]
        }
      ],
      "source": [
        "# output: (6,) => A shape of (6,) means the array is one-dimensional, containing 6 elements. For example:\n",
        "# A shape of (6, 1) indicates a two-dimensional array with 6 rows and 1 column. It's like a column vector in linear algebra. Example:\n",
        "# b = np.array([[1], [2], [0], [1], [0], [2]])\n",
        "# print(b.shape)  # Output: (6, 1)\n",
        "print(example_y_encoded.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "le5MJreAbW52"
      },
      "source": [
        "#### Label Encoding the \"Gender\" column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['Female', 'Female', 'Female', ..., 'Female', 'Male', 'Female'],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# before encoding gender\n",
        "# X is a two-diemnsionary array or DataFrame\n",
        "# [:,2] is an example of slicing. The colon : means \"select all rows\", and 2 after the comma refers to the third column (since Python indexing starts at 0).\n",
        "# So X[:, 2] means \"select all rows from the third column of X\"\n",
        "X[:,2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 1, 0], dtype=object)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "# See example above to understand how LabelEncoder really works - TLDR; it convert some string into numbers, i.e. all \"Male\" become 1 and \"Female\" becomes 0, etc.\n",
        "# Note that Label encoding will treat different capitalizations as distinct categories. The LabelEncoder in scikit-learn does not automatically lowercase or uppercase the data, so \"Male\" and \"male\" would be encoded as two different categories.\n",
        "\n",
        "# what this is doing is taking all the rows in X, and JUST the 3rd column (which is the gender) and running fit_transform through it, and reassigning back to all the rows.\n",
        "X[:, 2] = le.fit_transform(X[:, 2])\n",
        "# after encoding gender\n",
        "X[:, 2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CUxGZezpbMcb"
      },
      "source": [
        "#### One Hot Encoding the \"Geography\" column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### One Hot Encoding vs Label Encoding (why we can't use label encoding for the geograph variable)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Firstly, label encoding would turn `[\"Red\", \"Green\", \"Red\", \"Blue\", \"Blue\"]` into `array([2, 1, 2, 0, 0])` (integer encoding are assigned alphabetically by default)\n",
        "\n",
        "While, one hot encoding output may be\n",
        "```\n",
        "array([[1., 0., 0.],\n",
        "       [0., 1., 0.],\n",
        "       [1., 0., 0.],\n",
        "       [0., 0., 1.],\n",
        "       [0., 0., 1.]])\n",
        "```\n",
        "\n",
        "Why the difference?\n",
        "\n",
        "Issues with Label Encoding:\n",
        "\n",
        "- **Ordinality**: Label encoding assigns each unique category in a feature to a number. For example, if you have a feature \"color\" with values \"red\", \"green\", and \"blue\", these might be encoded as 0, 1, and 2 respectively. The problem is that many machine learning algorithms (especially those that are mathematically based) may interpret these values as ordinal data. So, they might consider \"blue\" (2) as having a higher value than \"green\" (1) and \"red\" (0). In reality, these are just distinct categories with no intrinsic order.\n",
        "\n",
        "- **Bias**: Since machine learning models might consider these values as ordinal, there’s a risk of introducing a bias. The model might incorrectly learn patterns that are influenced by the ordering of categories, even though such an order does not exist. This can lead to incorrect or suboptimal predictions.\n",
        "\n",
        "When One-Hot Encoding is Preferable:\n",
        "\n",
        "- One-hot encoding, on the other hand, mitigates these issues by creating a binary column for each category/label in the feature. It does not introduce ordinality or a bias because each category is represented equally. In the one-hot encoded form, each category is equally distant from the others, so the machine learning model won’t misinterpret the categorical data as ordinal.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['France', 'Spain', 'France', ..., 'France', 'Germany', 'France'],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# before one hot encoding\n",
        "X[:,1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.0 0.0 0.0 ... 0.0 1.0 0.0]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# The column indices is set to [1] which means to apply the transformer to column at index 1 of the dataset (i.e. column 2, which represents geography, since python index starts from 0)\n",
        "column_transformer = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1])], remainder='passthrough')\n",
        "\n",
        "X = np.array(column_transformer.fit_transform(X))\n",
        "\n",
        "# after one hot encoding - france, germany, spain etc. are converted to some multi-dimensional array\n",
        "print(X[:,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1.0, 0.0, 0.0, ..., 1, 1, 101348.88],\n",
              "       [0.0, 0.0, 1.0, ..., 0, 1, 112542.58],\n",
              "       [1.0, 0.0, 0.0, ..., 1, 0, 113931.57],\n",
              "       ...,\n",
              "       [1.0, 0.0, 0.0, ..., 0, 1, 42085.58],\n",
              "       [0.0, 1.0, 0.0, ..., 1, 0, 92888.52],\n",
              "       [1.0, 0.0, 0.0, ..., 1, 0, 38190.78]], dtype=object)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vHol938cW8zd"
      },
      "source": [
        "### Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# see your template if needed\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RE_FcHyfV3TQ"
      },
      "source": [
        "### Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# feature scaling is extremely important for deep learning; \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# instansitate a scaler object\n",
        "sc = StandardScaler()\n",
        "# we will do feature scaling all for columns and rows, even for those we already done label encoding on\n",
        "\n",
        "# Feature scaling (like min-max scaling, z-score normalization, etc.) should be fitted only on the training data, but it should be applied to both the training and testing datasets. Here’s the reasoning:\n",
        "# Fitting to the Training Set Only\n",
        "# Avoid Data Leakage: Fitting the scaler to the whole dataset, including the test set, means that you are using information from the test set to scale the training set. This is a type of data leakage, where information from the testing set leaks into the training process, potentially leading to overly optimistic performance estimates.\n",
        "\n",
        "# Simulation of Real-World Scenario: In a real-world scenario, you train your model on the available data (training set), and you evaluate its performance on new, unseen data (testing set). If you fit the scaler to the whole dataset, you’re violating this principle because you’re using information from the future (i.e., the test set) that wouldn’t be available at the time of training.\n",
        "\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.fit_transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-zfEzkRVXIwF"
      },
      "source": [
        "## Part 2 - Building the ANN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KvdeScabXtlB"
      },
      "source": [
        "### Initializing the ANN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "ann = tf.keras.models.Sequential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rP6urV6SX7kS"
      },
      "source": [
        "### Adding the input layer and the first hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# units stands for number of neurons\n",
        "# in deep learning, there's no rule of thumb for how many neurons - it is based on experimentation\n",
        "# we need to experiment based on hyperparameters - hyperparameters referring to parameters that won't be trained during the training process\n",
        "# maybe a decent number is between 5 and 30\n",
        "ann.add(tf.keras.layers.Dense(units=6, activation='relu'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BELWAc_8YJze"
      },
      "source": [
        "### Adding the second hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# we could also add dropout layer to prevent overfitting\n",
        "# or in Convultional Neural Network, we could add conv2D layer, which is a convolution layer\n",
        "ann.add(tf.keras.layers.Dense(units=6, activation='relu'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OyNEe6RXYcU4"
      },
      "source": [
        "### Adding the output layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# we use units = 1 for output because we are predicting a binary value - i.e. 1 or 0 for classification\n",
        "# if you are doing multi-class classification, i.e. Chocolate, Vanilla, or Strawberry, then you need 3 output neuron - and you'll then do one-hot-encoding like [0 0 1], [0 1 0], etc. to represent the classified output\n",
        "# we are also using the sigmoid activation function because that gives us the probability of how confident we are with the final output\n",
        "ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# if we're doing multi-class classification, we would use softmax activation function instead of sigmoid above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JT4u2S1_Y4WG"
      },
      "source": [
        "## Part 3 - Training the ANN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8GWlJChhY_ZI"
      },
      "source": [
        "### Compiling the ANN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### About `optimizer='adam'`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Role of the Optimizer: The optimizer is an algorithm or method used to change the attributes of the neural network, such as weights and learning rate, to reduce the losses. Optimizers help to minimize (or maximize) an objective function (the loss function) that is calculated as a function of the network's weights.\n",
        "\n",
        "How it Works: During training, the optimizer adjusts the weights based on the gradients of the loss function with respect to the weights. This process is iteratively done to find the best values of the weights to minimize the loss function, and hence improve the accuracy of the model.\n",
        "\n",
        "Common Types of Optimizers:\n",
        "\n",
        "1. Stochastic Gradient Descent (SGD): Uses a fixed learning rate and updates weights iteratively based on the training data.\n",
        "\n",
        "2. Momentum: Similar to SGD but takes into account the previous updates to smooth out the optimization landscape.\n",
        "\n",
        "3. Adagrad: Adapts the learning rate to the parameters, performing larger updates for infrequent and smaller updates for frequent parameters.\n",
        "\n",
        "4. RMSprop: Resolves Adagrad's radically diminishing learning rates by using a moving average of squared gradients.\n",
        "\n",
        "5. Adam (Adaptive Moment Estimation): Combines the ideas of Momentum and RMSprop, keeping track of an exponentially decaying average of past gradients and squared gradients.\n",
        "\n",
        "Choosing an Optimizer: The choice of an optimizer can significantly affect the performance of a neural network. It depends on the specific problem, the nature of the dataset, and the architecture of the network. Adam is a popular choice due to its effectiveness in a wide range of problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# we need to compile the ANN with an optimizer, a loss function, and a metric (i.e. accuracy)\n",
        "\n",
        "# TLDR; why we choose adam, why we choose binary_crossentropy, and why we choose accuracy is based on best practices\n",
        "\n",
        "# Adam is the most popular choice for optimizer\n",
        "# loss function: the way to compute the difference between the predicted value and the actual value\n",
        "# accuracy is your final metric\n",
        "# when you're doing binary classifiacation/i.e. predict a, binary output, you have to use binary_crossentropy\n",
        "# for binary classification, you have to use binary_crossentropy\n",
        "# for multiclass classification, you need to use categorical_crossentropy)\n",
        "\n",
        "# the metrics is purely for us to see during the training below; even if we omit it, there's no issue to the training. Of course, we add it so we can monitor the performance and finetune accordingly\n",
        "ann.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0QR_G5u7ZLSM"
      },
      "source": [
        "### Training the ANN on the Training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "250/250 [==============================] - 2s 2ms/step - loss: 0.5478 - accuracy: 0.7606\n",
            "Epoch 2/100\n",
            "212/250 [========================>.....] - ETA: 0s - loss: 0.4818 - accuracy: 0.7975"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "250/250 [==============================] - 1s 2ms/step - loss: 0.4801 - accuracy: 0.7960\n",
            "Epoch 3/100\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.4579 - accuracy: 0.7961\n",
            "Epoch 4/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.4435 - accuracy: 0.7968\n",
            "Epoch 5/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.4319 - accuracy: 0.7994\n",
            "Epoch 6/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.4212 - accuracy: 0.8048\n",
            "Epoch 7/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.4108 - accuracy: 0.8171\n",
            "Epoch 8/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.4009 - accuracy: 0.8266\n",
            "Epoch 9/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3916 - accuracy: 0.8334\n",
            "Epoch 10/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3816 - accuracy: 0.8404\n",
            "Epoch 11/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3722 - accuracy: 0.8439\n",
            "Epoch 12/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3647 - accuracy: 0.8495\n",
            "Epoch 13/100\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.3592 - accuracy: 0.8509\n",
            "Epoch 14/100\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.3552 - accuracy: 0.8535\n",
            "Epoch 15/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3521 - accuracy: 0.8560\n",
            "Epoch 16/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3497 - accuracy: 0.8572\n",
            "Epoch 17/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3487 - accuracy: 0.8547\n",
            "Epoch 18/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3471 - accuracy: 0.8566\n",
            "Epoch 19/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3460 - accuracy: 0.8579\n",
            "Epoch 20/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3454 - accuracy: 0.8591\n",
            "Epoch 21/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3446 - accuracy: 0.8565\n",
            "Epoch 22/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3440 - accuracy: 0.8576\n",
            "Epoch 23/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3430 - accuracy: 0.8587\n",
            "Epoch 24/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3425 - accuracy: 0.8601\n",
            "Epoch 25/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3419 - accuracy: 0.8594\n",
            "Epoch 26/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3410 - accuracy: 0.8615\n",
            "Epoch 27/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3409 - accuracy: 0.8615\n",
            "Epoch 28/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3404 - accuracy: 0.8609\n",
            "Epoch 29/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3397 - accuracy: 0.8610\n",
            "Epoch 30/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3395 - accuracy: 0.8606\n",
            "Epoch 31/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3393 - accuracy: 0.8611\n",
            "Epoch 32/100\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.3389 - accuracy: 0.8618\n",
            "Epoch 33/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3387 - accuracy: 0.8633\n",
            "Epoch 34/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3382 - accuracy: 0.8621\n",
            "Epoch 35/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3383 - accuracy: 0.8616\n",
            "Epoch 36/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3376 - accuracy: 0.8621\n",
            "Epoch 37/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3379 - accuracy: 0.8619\n",
            "Epoch 38/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3373 - accuracy: 0.8619\n",
            "Epoch 39/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3376 - accuracy: 0.8627\n",
            "Epoch 40/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3367 - accuracy: 0.8639\n",
            "Epoch 41/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3366 - accuracy: 0.8633\n",
            "Epoch 42/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3369 - accuracy: 0.8641\n",
            "Epoch 43/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3367 - accuracy: 0.8621\n",
            "Epoch 44/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3364 - accuracy: 0.8615\n",
            "Epoch 45/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3364 - accuracy: 0.8620\n",
            "Epoch 46/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3369 - accuracy: 0.8631\n",
            "Epoch 47/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3359 - accuracy: 0.8615\n",
            "Epoch 48/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3362 - accuracy: 0.8615\n",
            "Epoch 49/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3359 - accuracy: 0.8625\n",
            "Epoch 50/100\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.3356 - accuracy: 0.8633\n",
            "Epoch 51/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3359 - accuracy: 0.8619\n",
            "Epoch 52/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3360 - accuracy: 0.8637\n",
            "Epoch 53/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3352 - accuracy: 0.8636\n",
            "Epoch 54/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3355 - accuracy: 0.8627\n",
            "Epoch 55/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3352 - accuracy: 0.8619\n",
            "Epoch 56/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3350 - accuracy: 0.8622\n",
            "Epoch 57/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3351 - accuracy: 0.8626\n",
            "Epoch 58/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3352 - accuracy: 0.8635\n",
            "Epoch 59/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3347 - accuracy: 0.8639\n",
            "Epoch 60/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3350 - accuracy: 0.8625\n",
            "Epoch 61/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3349 - accuracy: 0.8630\n",
            "Epoch 62/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3343 - accuracy: 0.8650\n",
            "Epoch 63/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3347 - accuracy: 0.8640\n",
            "Epoch 64/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3350 - accuracy: 0.8616\n",
            "Epoch 65/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3342 - accuracy: 0.8639\n",
            "Epoch 66/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3345 - accuracy: 0.8630\n",
            "Epoch 67/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3344 - accuracy: 0.8640\n",
            "Epoch 68/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3341 - accuracy: 0.8644\n",
            "Epoch 69/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3339 - accuracy: 0.8648\n",
            "Epoch 70/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3335 - accuracy: 0.8643\n",
            "Epoch 71/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3339 - accuracy: 0.8645\n",
            "Epoch 72/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3341 - accuracy: 0.8645\n",
            "Epoch 73/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3336 - accuracy: 0.8629\n",
            "Epoch 74/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3335 - accuracy: 0.8658\n",
            "Epoch 75/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3332 - accuracy: 0.8648\n",
            "Epoch 76/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3334 - accuracy: 0.8637\n",
            "Epoch 77/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3335 - accuracy: 0.8654\n",
            "Epoch 78/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3335 - accuracy: 0.8651\n",
            "Epoch 79/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3331 - accuracy: 0.8648\n",
            "Epoch 80/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3334 - accuracy: 0.8648\n",
            "Epoch 81/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3328 - accuracy: 0.8670\n",
            "Epoch 82/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3328 - accuracy: 0.8636\n",
            "Epoch 83/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3329 - accuracy: 0.8668\n",
            "Epoch 84/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3331 - accuracy: 0.8654\n",
            "Epoch 85/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3328 - accuracy: 0.8656\n",
            "Epoch 86/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3324 - accuracy: 0.8652\n",
            "Epoch 87/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3325 - accuracy: 0.8656\n",
            "Epoch 88/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3324 - accuracy: 0.8648\n",
            "Epoch 89/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3324 - accuracy: 0.8652\n",
            "Epoch 90/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3322 - accuracy: 0.8646\n",
            "Epoch 91/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3319 - accuracy: 0.8664\n",
            "Epoch 92/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3321 - accuracy: 0.8665\n",
            "Epoch 93/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3322 - accuracy: 0.8676\n",
            "Epoch 94/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3327 - accuracy: 0.8668\n",
            "Epoch 95/100\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.3319 - accuracy: 0.8671\n",
            "Epoch 96/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3317 - accuracy: 0.8654\n",
            "Epoch 97/100\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.3324 - accuracy: 0.8668\n",
            "Epoch 98/100\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.3320 - accuracy: 0.8665\n",
            "Epoch 99/100\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.3323 - accuracy: 0.8649\n",
            "Epoch 100/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 0.3314 - accuracy: 0.8664\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x18582790d90>"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# .fit is to train the model\n",
        "# you start with epochs=100 as a general guide\n",
        "# we usually use batch_size = 32 as a general guide - i.e. mini batch gradient descent is a more precise term than stochastic gradient descent. (SGD uses a batch size of 1, while mini-batch gradient descent uses a batch size greater than 1.)\n",
        "# batch_size =32 means we are comparing 32 sets of predictions and 32 actual values, and then we update the weights\n",
        "# if u run this, you'll realize at around epoch=50, the loss will start to plateau/converge, so you can stop the training at around epoch=50\n",
        "ann.fit(X_train, y_train, batch_size=32, epochs=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Some notes about ML Model in Notebooks and Computer Memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TLDR; i trained the ANN and came back the next day and needed to retrain the whole thing; this is because the ANN exists only in memory while the notebook is running.\n",
        "\n",
        "As long as the Jupyter notebook kernel is running and has not been restarted or interrupted, the ANN remains in memory. This means you can train your ANN, leave the notebook open, and still access the ANN even after several hours, as long as the kernel is active.\n",
        "\n",
        "If I restart my computer or do anything that kestarts the kernel, then all variables (including my ANN) will be cleared from memory.\n",
        "\n",
        "Also, the amount of memory an ANN occupies depends on its size (number of layers and neurons) and the data types of its parameters. Larger networks with more parameters will naturally use more memory.\n",
        "\n",
        "therefore you need to save your ANN to a file if you want to keep it\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tJj5k2MxZga3"
      },
      "source": [
        "## Part 4 - Making the predictions and evaluating the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "84QFoqGYeXHL"
      },
      "source": [
        "### Predicting the result of a single observation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CGRo3eacgDdC"
      },
      "source": [
        "**Homework**\n",
        "\n",
        "Use our ANN model to predict if the customer with the following informations will leave the bank: \n",
        "\n",
        "Geography: France\n",
        "\n",
        "Credit Score: 600\n",
        "\n",
        "Gender: Male\n",
        "\n",
        "Age: 40 years old\n",
        "\n",
        "Tenure: 3 years\n",
        "\n",
        "Balance: \\$ 60000\n",
        "\n",
        "Number of Products: 2\n",
        "\n",
        "Does this customer have a credit card ? Yes\n",
        "\n",
        "Is this customer an Active Member: Yes\n",
        "\n",
        "Estimated Salary: \\$ 50000\n",
        "\n",
        "So, should we say goodbye to that customer ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **The need to reapply preprocessing step is SUPER interesting (check this out with gpt answer)\n",
        "https://chat.openai.com/c/0092e96d-3e68-47cc-9013-9311fdfe01d8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RowNumber                  1\n",
              "CustomerId          15634602\n",
              "Surname             Hargrave\n",
              "CreditScore              619\n",
              "Geography             France\n",
              "Gender                Female\n",
              "Age                       42\n",
              "Tenure                     2\n",
              "Balance                  0.0\n",
              "NumOfProducts              1\n",
              "HasCrCard                  1\n",
              "IsActiveMember             1\n",
              "EstimatedSalary    101348.88\n",
              "Exited                     1\n",
              "Name: 0, dtype: object"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.iloc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 182ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[0.04205877]], dtype=float32)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# the ann.predict method always expects a 2D array\n",
        "\n",
        "# Please check out this chatgpt convo out: https://chat.openai.com/c/0092e96d-3e68-47cc-9013-9311fdfe01d8\n",
        "# TLDR; when you predict a new data point, you NEED to perform all the same preprocessing step you did with your training data\n",
        "# This includes 1. One Hot Encoding some variables like Geography, 2. Label Encoding some variables like Gender, and 3. Perform Feature Scaling on the New Single Data point. Here, you NEED to use the same encoder and scaler in order to transform the new data point in the same way as the training data. (i.e. your training data's standard scaler was scaled according to the mean/median of the training data set, so you need to apply that to this new dataset)\n",
        "# **very important to APPLY and not fit_transform; fit_transform would recalculate the mean/median which is not what we want. we want to reuse the same stats from the training data\n",
        "# currently, this tutor just eyeball the encoding and replace \"France\" with 1, 0, 0 (which is correct) and \"Male\" with 1 which is also correct. But ideally you can check out the chatgpt convo above for the right way to do it.\n",
        "y_pred = ann.predict(sc.transform([[1, 0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]]))\n",
        "\n",
        "# because we indicated a sigmoid function in our output layer earlier, what we will get is the probability that the customer would leave the bank\n",
        "# churn baby: https://chat.openai.com/c/937de16f-6973-424e-818e-c9a24fab2e5f\n",
        "\n",
        "# this represnts the probabiltiy the customer would leave the bank\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[False]])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# you can, to some degree, use this metric to determine if the customer would leave the bank or not\n",
        "# this is just an abitrary threshold that you set (i.e. if your confidence rate >50% means he will leave)\n",
        "# in this case, the y_pred rate is 0.03 so the prediction is that the customer likely won't leave the bank\n",
        "y_pred > 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZhU1LTgPg-kH"
      },
      "source": [
        "**Solution**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wGjx94g2n7OV"
      },
      "source": [
        "Therefore, our ANN model predicts that this customer stays in the bank!\n",
        "\n",
        "**Important note 1:** Notice that the values of the features were all input in a double pair of square brackets. That's because the \"predict\" method always expects a 2D array as the format of its inputs. And putting our values into a double pair of square brackets makes the input exactly a 2D array.\n",
        "\n",
        "**Important note 2:** Notice also that the \"France\" country was not input as a string in the last column but as \"1, 0, 0\" in the first three columns. That's because of course the predict method expects the one-hot-encoded values of the state, and as we see in the first row of the matrix of features X, \"France\" was encoded as \"1, 0, 0\". And be careful to include these values in the first three columns, because the dummy variables are always created in the first columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u7yx47jPZt11"
      },
      "source": [
        "### Predicting the Test set results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 0s 972us/step\n",
            "[Prediction | Actual]\n",
            "[[0 0]\n",
            " [0 1]\n",
            " [0 0]\n",
            " ...\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]]\n"
          ]
        }
      ],
      "source": [
        "# again, using 0.5 as our threshold, we check if the customer is predicted to leave the bank\n",
        "y_pred = ann.predict(X_test) > 0.5\n",
        "# y_pred on the left; y_test on the right\n",
        "print('[Prediction | Actual]')\n",
        "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o0oyfLWoaEGw"
      },
      "source": [
        "### Making the Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1534   61]\n",
            " [ 209  196]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.865"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "\n",
        "# accuracy rate of 86%\n",
        "accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "artificial_neural_network.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
