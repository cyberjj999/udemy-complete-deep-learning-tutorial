{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3cbb7fRy-eyr"
      },
      "source": [
        "# Artificial Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8sNDnxE2-pwE"
      },
      "source": [
        "### Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "lxChR1Rk-umf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "uBTqR3nacj0e",
        "outputId": "4c0bd183-e424-429a-9fba-ceb841c06888"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.13.0'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AG3FQEch-yuA"
      },
      "source": [
        "## Part 1 - Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-4zq8Mza_D9O"
      },
      "source": [
        "### Importing the dataset\n",
        "This is why reading the source dataset is important, the acronym below stands for different things\n",
        "\n",
        "Extracted from the website: https://archive.ics.uci.edu/dataset/294/combined+cycle+power+plant\n",
        "\n",
        "Features consist of hourly average ambient variables\n",
        "- Temperature (T) in the range 1.81°C and 37.11°C,\n",
        "- Ambient Pressure (AP) in the range 992.89-1033.30 milibar,\n",
        "- Relative Humidity (RH) in the range 25.56% to 100.16%\n",
        "- Exhaust Vacuum (V) in teh range 25.36-81.56 cm Hg\n",
        "- Net hourly electrical energy output (EP) 420.26-495.76 MW\n",
        "The averages are taken from various sensors located around the plant that record the ambient variables every second. The variables are given without normalization. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AT</th>\n",
              "      <th>V</th>\n",
              "      <th>AP</th>\n",
              "      <th>RH</th>\n",
              "      <th>PE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14.96</td>\n",
              "      <td>41.76</td>\n",
              "      <td>1024.07</td>\n",
              "      <td>73.17</td>\n",
              "      <td>463.26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>25.18</td>\n",
              "      <td>62.96</td>\n",
              "      <td>1020.04</td>\n",
              "      <td>59.08</td>\n",
              "      <td>444.37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.11</td>\n",
              "      <td>39.40</td>\n",
              "      <td>1012.16</td>\n",
              "      <td>92.14</td>\n",
              "      <td>488.56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20.86</td>\n",
              "      <td>57.32</td>\n",
              "      <td>1010.24</td>\n",
              "      <td>76.64</td>\n",
              "      <td>446.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10.82</td>\n",
              "      <td>37.50</td>\n",
              "      <td>1009.23</td>\n",
              "      <td>96.62</td>\n",
              "      <td>473.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9563</th>\n",
              "      <td>16.65</td>\n",
              "      <td>49.69</td>\n",
              "      <td>1014.01</td>\n",
              "      <td>91.00</td>\n",
              "      <td>460.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9564</th>\n",
              "      <td>13.19</td>\n",
              "      <td>39.18</td>\n",
              "      <td>1023.67</td>\n",
              "      <td>66.78</td>\n",
              "      <td>469.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9565</th>\n",
              "      <td>31.32</td>\n",
              "      <td>74.33</td>\n",
              "      <td>1012.92</td>\n",
              "      <td>36.48</td>\n",
              "      <td>429.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9566</th>\n",
              "      <td>24.48</td>\n",
              "      <td>69.45</td>\n",
              "      <td>1013.86</td>\n",
              "      <td>62.39</td>\n",
              "      <td>435.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9567</th>\n",
              "      <td>21.60</td>\n",
              "      <td>62.52</td>\n",
              "      <td>1017.23</td>\n",
              "      <td>67.87</td>\n",
              "      <td>453.28</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9568 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         AT      V       AP     RH      PE\n",
              "0     14.96  41.76  1024.07  73.17  463.26\n",
              "1     25.18  62.96  1020.04  59.08  444.37\n",
              "2      5.11  39.40  1012.16  92.14  488.56\n",
              "3     20.86  57.32  1010.24  76.64  446.48\n",
              "4     10.82  37.50  1009.23  96.62  473.90\n",
              "...     ...    ...      ...    ...     ...\n",
              "9563  16.65  49.69  1014.01  91.00  460.03\n",
              "9564  13.19  39.18  1023.67  66.78  469.62\n",
              "9565  31.32  74.33  1012.92  36.48  429.57\n",
              "9566  24.48  69.45  1013.86  62.39  435.74\n",
              "9567  21.60  62.52  1017.23  67.87  453.28\n",
              "\n",
              "[9568 rows x 5 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = pd.read_excel('Dataset/Folds5x2_pp.xlsx')\n",
        "# AT is average temperature and PE is power output (the info at the website above is abit fucked.)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split dataset into matrix feature and dependent variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AT</th>\n",
              "      <th>V</th>\n",
              "      <th>AP</th>\n",
              "      <th>RH</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14.96</td>\n",
              "      <td>41.76</td>\n",
              "      <td>1024.07</td>\n",
              "      <td>73.17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>25.18</td>\n",
              "      <td>62.96</td>\n",
              "      <td>1020.04</td>\n",
              "      <td>59.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.11</td>\n",
              "      <td>39.40</td>\n",
              "      <td>1012.16</td>\n",
              "      <td>92.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20.86</td>\n",
              "      <td>57.32</td>\n",
              "      <td>1010.24</td>\n",
              "      <td>76.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10.82</td>\n",
              "      <td>37.50</td>\n",
              "      <td>1009.23</td>\n",
              "      <td>96.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9563</th>\n",
              "      <td>16.65</td>\n",
              "      <td>49.69</td>\n",
              "      <td>1014.01</td>\n",
              "      <td>91.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9564</th>\n",
              "      <td>13.19</td>\n",
              "      <td>39.18</td>\n",
              "      <td>1023.67</td>\n",
              "      <td>66.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9565</th>\n",
              "      <td>31.32</td>\n",
              "      <td>74.33</td>\n",
              "      <td>1012.92</td>\n",
              "      <td>36.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9566</th>\n",
              "      <td>24.48</td>\n",
              "      <td>69.45</td>\n",
              "      <td>1013.86</td>\n",
              "      <td>62.39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9567</th>\n",
              "      <td>21.60</td>\n",
              "      <td>62.52</td>\n",
              "      <td>1017.23</td>\n",
              "      <td>67.87</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9568 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         AT      V       AP     RH\n",
              "0     14.96  41.76  1024.07  73.17\n",
              "1     25.18  62.96  1020.04  59.08\n",
              "2      5.11  39.40  1012.16  92.14\n",
              "3     20.86  57.32  1010.24  76.64\n",
              "4     10.82  37.50  1009.23  96.62\n",
              "...     ...    ...      ...    ...\n",
              "9563  16.65  49.69  1014.01  91.00\n",
              "9564  13.19  39.18  1023.67  66.78\n",
              "9565  31.32  74.33  1012.92  36.48\n",
              "9566  24.48  69.45  1013.86  62.39\n",
              "9567  21.60  62.52  1017.23  67.87\n",
              "\n",
              "[9568 rows x 4 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# take everything but the dependent variable\n",
        "dataset.iloc[:, :-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# :-1 selects is index slicing, i.e. select all indexes from lower bound to -1 (exclude upper bound), so everything except last index\n",
        "X = dataset.iloc[:, :-1].values\n",
        "\n",
        "# iloc[]: This is a pandas DataFrame attribute used for integer-location based indexing, meaning you can select elements of the DataFrame by using integer indices.\n",
        "# In iloc[:, -1], the colon : in the first parameter signifies that we want all rows of the DataFrame.\n",
        "# -1: This means the last column of the DataFrame is being selected. In Python, indexing starts at 0, and negative indexing starts from -1 for the last element. So, -1 would mean the last column, -2 would mean the second last column, and so on.\n",
        "# values: This attribute is used to get a Numpy array representation of the DataFrame. It will give the values in the last column as a Numpy array.\n",
        "# i.e. first parameter is for rows and second parameter is for columns, you can do index slicing for each param\n",
        "y = dataset.iloc[:, -1].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([463.26, 444.37, 488.56, ..., 429.57, 435.74, 453.28])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VC6omXel_Up0"
      },
      "source": [
        "### Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# see your template if needed\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_mSLlAT9_eyI"
      },
      "source": [
        "## Part 2 - Building the Artificial Neural Network (ANN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is the ANN we will be building\n",
        "- We will have 4 features (each contributing to one neuron in the input layer)\n",
        "- We will have 2 hidden layers with 6 neurons each\n",
        "- Finally we will have the output layer, outputting the energy output prediction\n",
        "\n",
        "(Side note: why particularly 2 hidden layers with 6 neurons? The course author said this is just learned from his personal experience that this will bring the best results)\n",
        "\n",
        "![Alt text](ANN_Architecture.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CsBULd_f_wLY"
      },
      "source": [
        "### Initializing the ANN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# There are two types of artificial neural network\n",
        "# 1. Sequence of layers\n",
        "# 2. Computational graph (i.e. boltzmann machines - restricted boltzmann machine or deep boltzmann machines)\n",
        "# Here, we will use a sequence of layer based on the diagram we showed above\n",
        "\n",
        "# Instantiate object of the Sequential class, thus there are useful inbuilt methods inside\n",
        "# side note: keras used to be a separate library, but tensorflow 2 incorporated keras into its library\n",
        "ann = tf.keras.models.Sequential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iitAFJS_ABUn"
      },
      "source": [
        "### Adding the input layer and the first hidden layer\n",
        "\n",
        "\n",
        "- `tf.keras.layers.Dense(units=6, activation='relu')`: This creates a dense (fully connected) layer.\n",
        "\n",
        "    - `units=6`: This argument specifies that there will be 6 neurons (or units) in this dense layer.\n",
        "  \n",
        "    - `activation='relu'`: This argument sets the activation function for the dense layer to ReLU (Rectified Linear Activation). An activation function defines the output of a neuron given an input. The ReLU function outputs the input directly if it is positive; otherwise, it will output zero. Mathematically, it is defined as \\(f(x) = \\max(0, x)\\).\n",
        "\n",
        "So in simple terms, this line of code is adding a fully connected layer with 6 neurons and ReLU activation to the `ann` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# You initialized an ANN above, but now you need to add in the different layers yourself.\n",
        "# You can use the add function to add layers to a neural network\n",
        "\n",
        "# The actual layer we want to create is under a class called 'Dense'. However, to access the 'Dense' class, we need to go through the tf library, then the keras library, then the layers module, then the Dense class. i.e. tf.keras.layers.Dense\n",
        "# The intuition of the word dense is because of the \"high density\" connection between the layers, i.e. a lot of lines intersecting, fully connected layers\n",
        "\n",
        "# units refer to the number of neurons we want in the layer - we have 6 in this case\n",
        "ann.add(tf.keras.layers.Dense(units=6, activation='relu'))\n",
        "\n",
        "# The activation function for the dense layer is set to ReLU (Rectified Linear Activation). An activation function defines the output of a neuron given an input. The ReLU function outputs the input directly if it is positive; otherwise, it will output zero. Mathematically, it is defined as \\(f(x) = \\max(0, x)\\).\n",
        "\n",
        "\n",
        "# Input layer is instantitated automatically\n",
        "# Side note: notice how we didn't need to instantiate that there are four features in the input layer, this is because tensorflow will automatically detect the number of features in the input layer when we pass in the datasest later"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-lb4kK_wAKbs"
      },
      "source": [
        "### Adding the second hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "ann.add(tf.keras.layers.Dense(units=6, activation='relu'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jwMOmKb3AdBY"
      },
      "source": [
        "### Adding the output layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Usually, for regression problems, you have a single neuron in the output layer with no activation function or a linear activation function.\n",
        "# Rationale is that in the cases of predicting continuous value like housing price or temperature, we want the network to be able to predict a range of values as output, without any constraint. (other activation function like sigmoid or Relu will constrain the output to a certain range like between 0 and 1, etc.)\n",
        "# side note: if you want to predict other kind of outputs, you can use other activation function - can ask ChatGPT\n",
        "ann.add(tf.keras.layers.Dense(units=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fq7e4fF6A1yy"
      },
      "source": [
        "## Part 3 - Training the ANN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qDeylAs2An25"
      },
      "source": [
        "### Compiling the ANN with an optimizer and loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimizer: the tool which you'll use to perform stochastic gradient descent\n",
        "# It will essentially calculate the loss function and then update the weights accordingly to minimize loss\n",
        "\n",
        "# compile is a method to configure the model for training.\n",
        "# Inside the compile method, there are two important parameters being set: optimizer and loss.\n",
        "\n",
        "# optimizer='adam': The optimizer is responsible for updating the weights of the neurons in the network to minimize the loss function. 'Adam' is a specific type of optimization algorithm that is often used because it is efficient and has low memory requirements. It stands for \"Adaptive Moment Estimation\" and is known for its effectiveness in practice and efficiency in computation.\n",
        "\n",
        "# loss='mean_squared_error': The loss function, or cost function, is a measure of how well the model is doing, and the training process aims to minimize this value. 'Mean Squared Error' is a common loss function used for regression problems. It calculates the average of the squares of the differences between predicted and actual values.\n",
        "\n",
        "# As for the rationale WHY we use adam specifically and loss specifically.... ask chatgpt...\n",
        "ann.compile(optimizer='adam', loss='mean_squared_error')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YjVuiybYOo7r"
      },
      "source": [
        "### Training the ANN model on the Training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "240/240 [==============================] - 1s 714us/step - loss: 83685.5547\n",
            "Epoch 2/100\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 345.8811\n",
            "Epoch 3/100\n",
            "240/240 [==============================] - 0s 697us/step - loss: 326.4513\n",
            "Epoch 4/100\n",
            "240/240 [==============================] - 0s 684us/step - loss: 303.5670\n",
            "Epoch 5/100\n",
            "240/240 [==============================] - 0s 695us/step - loss: 278.5840\n",
            "Epoch 6/100\n",
            "240/240 [==============================] - 0s 729us/step - loss: 253.2430\n",
            "Epoch 7/100\n",
            "240/240 [==============================] - 0s 693us/step - loss: 227.5685\n",
            "Epoch 8/100\n",
            "240/240 [==============================] - 0s 773us/step - loss: 202.3802\n",
            "Epoch 9/100\n",
            "240/240 [==============================] - 0s 967us/step - loss: 177.8712\n",
            "Epoch 10/100\n",
            "240/240 [==============================] - 0s 672us/step - loss: 155.3874\n",
            "Epoch 11/100\n",
            "240/240 [==============================] - 0s 710us/step - loss: 134.4253\n",
            "Epoch 12/100\n",
            "240/240 [==============================] - 0s 712us/step - loss: 115.4400\n",
            "Epoch 13/100\n",
            "240/240 [==============================] - 0s 763us/step - loss: 99.4576\n",
            "Epoch 14/100\n",
            "240/240 [==============================] - 0s 685us/step - loss: 85.1850\n",
            "Epoch 15/100\n",
            "240/240 [==============================] - 0s 700us/step - loss: 73.6368\n",
            "Epoch 16/100\n",
            "240/240 [==============================] - 0s 687us/step - loss: 65.1956\n",
            "Epoch 17/100\n",
            "240/240 [==============================] - 0s 690us/step - loss: 57.9552\n",
            "Epoch 18/100\n",
            "240/240 [==============================] - 0s 683us/step - loss: 53.0546\n",
            "Epoch 19/100\n",
            "240/240 [==============================] - 0s 697us/step - loss: 49.1802\n",
            "Epoch 20/100\n",
            "240/240 [==============================] - 0s 730us/step - loss: 46.5683\n",
            "Epoch 21/100\n",
            "240/240 [==============================] - 0s 778us/step - loss: 44.8127\n",
            "Epoch 22/100\n",
            "240/240 [==============================] - 0s 678us/step - loss: 42.9916\n",
            "Epoch 23/100\n",
            "240/240 [==============================] - 0s 670us/step - loss: 41.8403\n",
            "Epoch 24/100\n",
            "240/240 [==============================] - 0s 682us/step - loss: 40.2887\n",
            "Epoch 25/100\n",
            "240/240 [==============================] - 0s 729us/step - loss: 39.2486\n",
            "Epoch 26/100\n",
            "240/240 [==============================] - 0s 704us/step - loss: 37.8663\n",
            "Epoch 27/100\n",
            "240/240 [==============================] - 0s 764us/step - loss: 37.0587\n",
            "Epoch 28/100\n",
            "240/240 [==============================] - 0s 742us/step - loss: 36.6443\n",
            "Epoch 29/100\n",
            "240/240 [==============================] - 0s 692us/step - loss: 35.7831\n",
            "Epoch 30/100\n",
            "240/240 [==============================] - 0s 704us/step - loss: 35.0224\n",
            "Epoch 31/100\n",
            "240/240 [==============================] - 0s 826us/step - loss: 33.7494\n",
            "Epoch 32/100\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 33.0850\n",
            "Epoch 33/100\n",
            "240/240 [==============================] - 0s 1ms/step - loss: 32.2590\n",
            "Epoch 34/100\n",
            "240/240 [==============================] - 0s 885us/step - loss: 31.5078\n",
            "Epoch 35/100\n",
            "240/240 [==============================] - 0s 795us/step - loss: 30.6563\n",
            "Epoch 36/100\n",
            "240/240 [==============================] - 0s 824us/step - loss: 31.2187\n",
            "Epoch 37/100\n",
            "240/240 [==============================] - 0s 749us/step - loss: 30.9739\n",
            "Epoch 38/100\n",
            "240/240 [==============================] - 0s 773us/step - loss: 29.6811\n",
            "Epoch 39/100\n",
            "240/240 [==============================] - 0s 736us/step - loss: 30.0979\n",
            "Epoch 40/100\n",
            "240/240 [==============================] - 0s 777us/step - loss: 29.1234\n",
            "Epoch 41/100\n",
            "240/240 [==============================] - 0s 732us/step - loss: 28.9416\n",
            "Epoch 42/100\n",
            "240/240 [==============================] - 0s 713us/step - loss: 28.5717\n",
            "Epoch 43/100\n",
            "240/240 [==============================] - 0s 685us/step - loss: 28.3112\n",
            "Epoch 44/100\n",
            "240/240 [==============================] - 0s 759us/step - loss: 28.0107\n",
            "Epoch 45/100\n",
            "240/240 [==============================] - 0s 773us/step - loss: 28.1282\n",
            "Epoch 46/100\n",
            "240/240 [==============================] - 0s 760us/step - loss: 28.0135\n",
            "Epoch 47/100\n",
            "240/240 [==============================] - 0s 771us/step - loss: 28.1466\n",
            "Epoch 48/100\n",
            "240/240 [==============================] - 0s 780us/step - loss: 27.8098\n",
            "Epoch 49/100\n",
            "240/240 [==============================] - 0s 709us/step - loss: 27.5775\n",
            "Epoch 50/100\n",
            "240/240 [==============================] - 0s 763us/step - loss: 27.1783\n",
            "Epoch 51/100\n",
            "240/240 [==============================] - 0s 894us/step - loss: 27.5926\n",
            "Epoch 52/100\n",
            "240/240 [==============================] - 0s 737us/step - loss: 27.8814\n",
            "Epoch 53/100\n",
            "240/240 [==============================] - 0s 742us/step - loss: 27.1507\n",
            "Epoch 54/100\n",
            "240/240 [==============================] - 0s 738us/step - loss: 26.8117\n",
            "Epoch 55/100\n",
            "240/240 [==============================] - 0s 715us/step - loss: 27.2235\n",
            "Epoch 56/100\n",
            "240/240 [==============================] - 0s 707us/step - loss: 26.4437\n",
            "Epoch 57/100\n",
            "240/240 [==============================] - 0s 740us/step - loss: 26.9575\n",
            "Epoch 58/100\n",
            "240/240 [==============================] - 0s 734us/step - loss: 26.5853\n",
            "Epoch 59/100\n",
            "240/240 [==============================] - 0s 744us/step - loss: 27.4720\n",
            "Epoch 60/100\n",
            "240/240 [==============================] - 0s 702us/step - loss: 27.9344\n",
            "Epoch 61/100\n",
            "240/240 [==============================] - 0s 648us/step - loss: 26.7799\n",
            "Epoch 62/100\n",
            "240/240 [==============================] - 0s 658us/step - loss: 27.2636\n",
            "Epoch 63/100\n",
            "240/240 [==============================] - 0s 670us/step - loss: 27.1984\n",
            "Epoch 64/100\n",
            "240/240 [==============================] - 0s 821us/step - loss: 27.0985\n",
            "Epoch 65/100\n",
            "240/240 [==============================] - 0s 722us/step - loss: 26.8027\n",
            "Epoch 66/100\n",
            "240/240 [==============================] - 0s 681us/step - loss: 26.6529\n",
            "Epoch 67/100\n",
            "240/240 [==============================] - 0s 766us/step - loss: 26.6910\n",
            "Epoch 68/100\n",
            "240/240 [==============================] - 0s 723us/step - loss: 26.7950\n",
            "Epoch 69/100\n",
            "240/240 [==============================] - 0s 749us/step - loss: 27.7473\n",
            "Epoch 70/100\n",
            "240/240 [==============================] - 0s 778us/step - loss: 27.1812\n",
            "Epoch 71/100\n",
            "240/240 [==============================] - 0s 684us/step - loss: 27.2549\n",
            "Epoch 72/100\n",
            "240/240 [==============================] - 0s 720us/step - loss: 27.1369\n",
            "Epoch 73/100\n",
            "240/240 [==============================] - 0s 691us/step - loss: 26.6335\n",
            "Epoch 74/100\n",
            "240/240 [==============================] - 0s 693us/step - loss: 26.7811\n",
            "Epoch 75/100\n",
            "240/240 [==============================] - 0s 787us/step - loss: 27.2178\n",
            "Epoch 76/100\n",
            "240/240 [==============================] - 0s 699us/step - loss: 26.7742\n",
            "Epoch 77/100\n",
            "240/240 [==============================] - 0s 676us/step - loss: 26.6540\n",
            "Epoch 78/100\n",
            "240/240 [==============================] - 0s 810us/step - loss: 27.5022\n",
            "Epoch 79/100\n",
            "240/240 [==============================] - 0s 687us/step - loss: 26.9933\n",
            "Epoch 80/100\n",
            "240/240 [==============================] - 0s 678us/step - loss: 26.6573\n",
            "Epoch 81/100\n",
            "240/240 [==============================] - 0s 714us/step - loss: 27.0405\n",
            "Epoch 82/100\n",
            "240/240 [==============================] - 0s 728us/step - loss: 26.9509\n",
            "Epoch 83/100\n",
            "240/240 [==============================] - 0s 737us/step - loss: 27.0036\n",
            "Epoch 84/100\n",
            "240/240 [==============================] - 0s 697us/step - loss: 26.8726\n",
            "Epoch 85/100\n",
            "240/240 [==============================] - 0s 686us/step - loss: 27.0616\n",
            "Epoch 86/100\n",
            "240/240 [==============================] - 0s 772us/step - loss: 26.9659\n",
            "Epoch 87/100\n",
            "240/240 [==============================] - 0s 686us/step - loss: 26.5727\n",
            "Epoch 88/100\n",
            "240/240 [==============================] - 0s 699us/step - loss: 26.4429\n",
            "Epoch 89/100\n",
            "240/240 [==============================] - 0s 692us/step - loss: 27.0636\n",
            "Epoch 90/100\n",
            "240/240 [==============================] - 0s 676us/step - loss: 26.9023\n",
            "Epoch 91/100\n",
            "240/240 [==============================] - 0s 686us/step - loss: 28.4264\n",
            "Epoch 92/100\n",
            "240/240 [==============================] - 0s 700us/step - loss: 27.0196\n",
            "Epoch 93/100\n",
            "240/240 [==============================] - 0s 726us/step - loss: 26.6011\n",
            "Epoch 94/100\n",
            "240/240 [==============================] - 0s 675us/step - loss: 26.8202\n",
            "Epoch 95/100\n",
            "240/240 [==============================] - 0s 753us/step - loss: 26.7231\n",
            "Epoch 96/100\n",
            "240/240 [==============================] - 0s 759us/step - loss: 26.8522\n",
            "Epoch 97/100\n",
            "240/240 [==============================] - 0s 727us/step - loss: 27.2383\n",
            "Epoch 98/100\n",
            "240/240 [==============================] - 0s 734us/step - loss: 26.7473\n",
            "Epoch 99/100\n",
            "240/240 [==============================] - 0s 776us/step - loss: 26.9227\n",
            "Epoch 100/100\n",
            "240/240 [==============================] - 0s 737us/step - loss: 26.6555\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x21051bb7250>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# .fit is to train the model\n",
        "# you start with epochs=100 as a general guide\n",
        "# we usually use batch_size = 32 as a general guide - i.e. mini batch gradient descent is a more precise term than stochastic gradient descent. (SGD uses a batch size of 1, while mini-batch gradient descent uses a batch size greater than 1.)\n",
        "# if u run this, you'll realize at around epoch=50, the loss will start to plateau/converge, so you can stop the training at around epoch=50\n",
        "ann.fit(X_train, y_train, batch_size=32, epochs=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4 - Making Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0H0zKKNEBLD5"
      },
      "source": [
        "### Predicting the results of the Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "60/60 [==============================] - 0s 576us/step\n"
          ]
        }
      ],
      "source": [
        "# using our trained ANN to do prediction\n",
        "y_pred = ann.predict(X_test)\n",
        "\n",
        "# ensure 2dp output when printing - just to make sure output is neater\n",
        "np.set_printoptions(precision=2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1914, 1)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[430.99, 431.23],\n",
              "       [462.01, 460.01],\n",
              "       [465.5 , 461.14],\n",
              "       ...,\n",
              "       [472.72, 473.26],\n",
              "       [439.58, 438.  ],\n",
              "       [458.75, 463.28]])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# the issue is that y_pred is horizontal\n",
        "side_by_side_comparison = np.concatenate((y_pred.reshape(len(y_pred), 1), y_test.reshape(len(y_test), 1)), 1)\n",
        "# left side is predicted value, right side is actual value\n",
        "# they are pretty damn close - 431.42 vs 431.23\n",
        "# excellent performance.\n",
        "side_by_side_comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# this code seems kinda wrong\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# reshaped_y_pred = y_pred.reshape(len(y_pred), 1)\n",
        "# reshaped_y_test = y_test.reshape(len(y_test), 1)\n",
        "# plt.scatter(reshaped_y_test, reshaped_y_pred)\n",
        "# plt.xlabel('True Values')\n",
        "# plt.ylabel('Predictions')\n",
        "# plt.axis('equal')\n",
        "# plt.axis('square')\n",
        "# plt.plot([-100, 100], [-100, 100], c='red')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Artificial Neural Network",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
